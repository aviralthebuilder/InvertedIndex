{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4041\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1567441136559)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 2\n",
       "y: Int = 3\n",
       "res0: Int = 5\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = 2\n",
    "val y = 3\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@7a165ee3\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| _1| _2| _3|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  6|  7|  8|\n",
      "|  9| 19| 10|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(Int, Int, Int)] = List((1,2,3), (4,5,6), (6,7,8), (9,19,10))\n",
       "ds: org.apache.spark.sql.Dataset[(Int, Int, Int)] = [_1: int, _2: int ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((1,2,3), (4,5,6), (6,7,8), (9,19,10))\n",
    "val ds = spark.createDataset(data)\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = ../../Desktop/bbcsports/bbc_sports_articles\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"../../Desktop/bbcsports/bbc_sports_articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputData: org.apache.spark.rdd.RDD[(String, String)] = ../../Desktop/bbcsports/bbc_sports_articles MapPartitionsRDD[1] at wholeTextFiles at <console>:27\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputData = sc.wholeTextFiles(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(String, String)] =\n",
       "Array((file:/Users/aviralsharma/Desktop/bbcsports/bbc_sports_articles/289.txt,\"Wilkinson targets Ireland\n",
       "\n",
       "Jonny Wilkinson says he is \"massively positive\" about his chances of playing some part in the Six Nations.\n",
       "\n",
       "A knee injury has ruled him out of England's first two matches against Wales and France, but he is hoping to feature against Ireland on 27 February. \"I'm coming on really well. The physios at England and Newcastle are as happy as they could possibly be at this point.\" said the England fly-half. Wilkinson has not played for England since their 2003 World Cup final win. \"The injury is to my left leg so, as a kicker, I need to make sure I'm really right so I can play my usual game.\"\n",
       "\n",
       "\"All I can do is try to get back. It (the knee injury) happened t..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [_1: string, _2: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|                  _2|\n",
      "+--------------------+--------------------+\n",
      "|file:/Users/avira...|Wilkinson targets...|\n",
      "|file:/Users/avira...|England 17-18 Fra...|\n",
      "|file:/Users/avira...|Moody joins up wi...|\n",
      "|file:/Users/avira...|McIlroy wins 800m...|\n",
      "|file:/Users/avira...|Pavey focuses on ...|\n",
      "|file:/Users/avira...|Greek pair set fo...|\n",
      "|file:/Users/avira...|Richardson ends j...|\n",
      "|file:/Users/avira...|Freeman considers...|\n",
      "|file:/Users/avira...|Sri Lanka squad s...|\n",
      "|file:/Users/avira...|Davenport dismant...|\n",
      "|file:/Users/avira...|Scotland v Italy ...|\n",
      "|file:/Users/avira...|Dallaglio his own...|\n",
      "|file:/Users/avira...|Capriati to miss ...|\n",
      "|file:/Users/avira...|Rusedski forced o...|\n",
      "|file:/Users/avira...|Martinez sees off...|\n",
      "|file:/Users/avira...|Woodward eyes Bre...|\n",
      "|file:/Users/avira...|Angry Williams re...|\n",
      "|file:/Users/avira...|Federer forced to...|\n",
      "|file:/Users/avira...|Sculthorpe wants ...|\n",
      "|file:/Users/avira...|Sri Lanka resched...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[(String, String)] = [_1: string, _2: string]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//creating data set from an RDD\n",
    "val ds = spark.createDataset(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|                  _2|\n",
      "+--------------------+--------------------+\n",
      "|file:/Users/avira...|Wilkinson targets...|\n",
      "|file:/Users/avira...|England 17-18 Fra...|\n",
      "|file:/Users/avira...|Moody joins up wi...|\n",
      "|file:/Users/avira...|McIlroy wins 800m...|\n",
      "|file:/Users/avira...|Pavey focuses on ...|\n",
      "|file:/Users/avira...|Greek pair set fo...|\n",
      "|file:/Users/avira...|Richardson ends j...|\n",
      "|file:/Users/avira...|Freeman considers...|\n",
      "|file:/Users/avira...|Sri Lanka squad s...|\n",
      "|file:/Users/avira...|Davenport dismant...|\n",
      "|file:/Users/avira...|Scotland v Italy ...|\n",
      "|file:/Users/avira...|Dallaglio his own...|\n",
      "|file:/Users/avira...|Capriati to miss ...|\n",
      "|file:/Users/avira...|Rusedski forced o...|\n",
      "|file:/Users/avira...|Martinez sees off...|\n",
      "|file:/Users/avira...|Woodward eyes Bre...|\n",
      "|file:/Users/avira...|Angry Williams re...|\n",
      "|file:/Users/avira...|Federer forced to...|\n",
      "|file:/Users/avira...|Sculthorpe wants ...|\n",
      "|file:/Users/avira...|Sri Lanka resched...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial: org.apache.spark.sql.Dataset[(Array[String], String)] = [_1: array<string>, _2: string]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial = ds.map(x => (x._2.split(\" \"), x._1.replace(\"file:/Users/aviralsharma/Desktop/bbcsports/bbc_sports_articles/\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                  _1|     _2|\n",
      "+--------------------+-------+\n",
      "|[Wilkinson, targe...|289.txt|\n",
      "|[England, 17-18, ...|262.txt|\n",
      "|[Moody, joins, up...|276.txt|\n",
      "|[McIlroy, wins, 8...|060.txt|\n",
      "|[Pavey, focuses, ...|074.txt|\n",
      "|[Greek, pair, set...|048.txt|\n",
      "|[Richardson, ends...|114.txt|\n",
      "|[Freeman, conside...|100.txt|\n",
      "|[Sri, Lanka, squa...|128.txt|\n",
      "|[Davenport, disma...|470.txt|\n",
      "|[Scotland, v, Ita...|316.txt|\n",
      "|[Dallaglio, his, ...|302.txt|\n",
      "|[Capriati, to, mi...|464.txt|\n",
      "|[Rusedski, forced...|458.txt|\n",
      "|[Martinez, sees, ...|459.txt|\n",
      "|[Woodward, eyes, ...|303.txt|\n",
      "|[Angry, Williams,...|465.txt|\n",
      "|[Federer, forced,...|471.txt|\n",
      "|[Sculthorpe, want...|317.txt|\n",
      "|[Sri, Lanka, resc...|129.txt|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": " error: incomplete input",
     "output_type": "error",
     "traceback": [
      "<console>: error: incomplete input"
     ]
    }
   ],
   "source": [
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial2: org.apache.spark.sql.Dataset[(Array[String], String, Int)] = [_1: array<string>, _2: string ... 1 more field]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial2 = trial.map(x => (x._1, x._2, x._1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---+\n",
      "|                  _1|     _2| _3|\n",
      "+--------------------+-------+---+\n",
      "|[Wilkinson, targe...|289.txt|249|\n",
      "|[England, 17-18, ...|262.txt|665|\n",
      "|[Moody, joins, up...|276.txt|455|\n",
      "|[McIlroy, wins, 8...|060.txt|374|\n",
      "|[Pavey, focuses, ...|074.txt|129|\n",
      "|[Greek, pair, set...|048.txt|418|\n",
      "|[Richardson, ends...|114.txt|339|\n",
      "|[Freeman, conside...|100.txt|241|\n",
      "|[Sri, Lanka, squa...|128.txt|301|\n",
      "|[Davenport, disma...|470.txt|486|\n",
      "|[Scotland, v, Ita...|316.txt|413|\n",
      "|[Dallaglio, his, ...|302.txt|736|\n",
      "|[Capriati, to, mi...|464.txt|127|\n",
      "|[Rusedski, forced...|458.txt|128|\n",
      "|[Martinez, sees, ...|459.txt|163|\n",
      "|[Woodward, eyes, ...|303.txt|219|\n",
      "|[Angry, Williams,...|465.txt|286|\n",
      "|[Federer, forced,...|471.txt|188|\n",
      "|[Sculthorpe, want...|317.txt|427|\n",
      "|[Sri, Lanka, resc...|129.txt|434|\n",
      "+--------------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "segregrate: (words: Array[String], doc: String, totalWords: Int)scala.collection.mutable.ListBuffer[(String, String, Int)]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def segregrate(words:Array[String], doc:String, totalWords:Int) :scala.collection.mutable.ListBuffer[(String, String, Int)] = {\n",
    "    val listTemp = scala.collection.mutable.ListBuffer.empty[(String, String, Int)]\n",
    "    for (word <- words){\n",
    "        //if (word,doc) not in listTemp:\n",
    "        listTemp += ((word, doc, totalWords))\n",
    "    }\n",
    "    return listTemp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d: scala.collection.mutable.ListBuffer[(String, String, Int)] = ListBuffer((hi,bi,9), (hi,bi,10))\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = scala.collection.mutable.ListBuffer((\"hi\", \"bi\", 9), (\"hi\", \"bi\", 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: d.type = ListBuffer((hi,bi,9), (hi,bi,10), (hi,bi,11))\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d += ((\"hi\", \"bi\", 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Class[_ <: scala.collection.mutable.ListBuffer[(String, String, Int)]] = class scala.collection.mutable.ListBuffer\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.getClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial3: org.apache.spark.sql.Dataset[(String, String, Int)] = [_1: string, _2: string ... 1 more field]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial3 = trial2.flatMap(x => segregrate(x._1, x._2, x._3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+---+\n",
      "|            _1|     _2| _3|\n",
      "+--------------+-------+---+\n",
      "|     Wilkinson|289.txt|249|\n",
      "|       targets|289.txt|249|\n",
      "|Ireland\n",
      "\n",
      "Jonny|289.txt|249|\n",
      "|     Wilkinson|289.txt|249|\n",
      "|          says|289.txt|249|\n",
      "|            he|289.txt|249|\n",
      "|            is|289.txt|249|\n",
      "|    \"massively|289.txt|249|\n",
      "|     positive\"|289.txt|249|\n",
      "|         about|289.txt|249|\n",
      "|           his|289.txt|249|\n",
      "|       chances|289.txt|249|\n",
      "|            of|289.txt|249|\n",
      "|       playing|289.txt|249|\n",
      "|          some|289.txt|249|\n",
      "|          part|289.txt|249|\n",
      "|            in|289.txt|249|\n",
      "|           the|289.txt|249|\n",
      "|           Six|289.txt|249|\n",
      "|   Nations.\n",
      "\n",
      "A|289.txt|249|\n",
      "+--------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial4: org.apache.spark.sql.Dataset[((String, String, Int), Int)] = [_1: struct<_1: string, _2: string ... 1 more field>, _2: int]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial4 = trial3.map(x => ((x._1, x._2, x._3), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                  _1| _2|\n",
      "+--------------------+---+\n",
      "|[Wilkinson, 289.t...|  1|\n",
      "|[targets, 289.txt...|  1|\n",
      "|[Ireland\n",
      "\n",
      "Jonny, ...|  1|\n",
      "|[Wilkinson, 289.t...|  1|\n",
      "|[says, 289.txt, 249]|  1|\n",
      "|  [he, 289.txt, 249]|  1|\n",
      "|  [is, 289.txt, 249]|  1|\n",
      "|[\"massively, 289....|  1|\n",
      "|[positive\", 289.t...|  1|\n",
      "|[about, 289.txt, ...|  1|\n",
      "| [his, 289.txt, 249]|  1|\n",
      "|[chances, 289.txt...|  1|\n",
      "|  [of, 289.txt, 249]|  1|\n",
      "|[playing, 289.txt...|  1|\n",
      "|[some, 289.txt, 249]|  1|\n",
      "|[part, 289.txt, 249]|  1|\n",
      "|  [in, 289.txt, 249]|  1|\n",
      "| [the, 289.txt, 249]|  1|\n",
      "| [Six, 289.txt, 249]|  1|\n",
      "|[Nations.\n",
      "\n",
      "A, 289...|  1|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial5: org.apache.spark.sql.DataFrame = [_1: struct<_1: string, _2: string ... 1 more field>, _2: bigint]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial5 = trial4.groupBy('_1).agg(sum('_2) as \"_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                  _1| _2|\n",
      "+--------------------+---+\n",
      "|[seeing, 262.txt,...|  1|\n",
      "|   [F, 262.txt, 665]|  2|\n",
      "|[after, 276.txt, ...|  1|\n",
      "|  [up, 060.txt, 374]|  1|\n",
      "|[unless, 048.txt,...|  1|\n",
      "|[later, 048.txt, ...|  1|\n",
      "|[average, 114.txt...|  1|\n",
      "|[has,, 100.txt, 241]|  1|\n",
      "|[after, 470.txt, ...|  3|\n",
      "|[Johnson,, 302.tx...|  1|\n",
      "|[that, 459.txt, 163]|  1|\n",
      "|[\"I've, 303.txt, ...|  1|\n",
      "|[switch, 317.txt,...|  1|\n",
      "|[understand, 317....|  1|\n",
      "|[capability, 115....|  1|\n",
      "|[never,, 049.txt,...|  1|\n",
      "|[trial, 049.txt, ...|  1|\n",
      "|[men's, 075.txt, ...|  4|\n",
      "| [aim, 061.txt, 653]|  1|\n",
      "|[against, 249.txt...|  2|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "26: error: value DataSet is not a member of org.apache.spark.sql.DataFrame",
     "output_type": "error",
     "traceback": [
      "<console>:26: error: value DataSet is not a member of org.apache.spark.sql.DataFrame",
      "       val rdd = trial5.DataSet",
      "                        ^",
      ""
     ]
    }
   ],
   "source": [
    "val rdd = trial5.DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[org.apache.spark.sql.Row] =\n",
       "Array([[seeing,262.txt,665],1], [[F,262.txt,665],2], [[after,276.txt,455],1], [[up,060.txt,374],1], [[unless,048.txt,418],1], [[later,048.txt,418],1], [[average,114.txt,339],1], [[has,,100.txt,241],1], [[after,470.txt,486],3], [[Johnson,,302.txt,736],1], [[that,459.txt,163],1], [[\"I've,303.txt,219],1], [[switch,317.txt,427],1], [[understand,317.txt,427],1], [[capability,115.txt,527],1], [[never,,049.txt,453],1], [[trial,049.txt,453],1], [[men's,075.txt,496],4], [[aim,061.txt,653],1], [[against,249.txt,464],2], [[August,,088.txt,454],1], [[this,063.txt,274],1], [[wickets,103.txt,361],1], [[despite,103.txt,361],1], [[South,103.txt,361],1], [[Paul,117.txt,327],1], [[2003,,117.txt,327],1], [[won,117.txt,327],1], [[else,117.txt,327],1], [[out,301.txt,..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial6: org.apache.spark.sql.Dataset[((String, String, Int), BigInt)] = [_1: struct<_1: string, _2: string ... 1 more field>, _2: bigint]\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial6 = trial5.as[((String, String, Int), BigInt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                  _1| _2|\n",
      "+--------------------+---+\n",
      "|[seeing, 262.txt,...|  1|\n",
      "|   [F, 262.txt, 665]|  2|\n",
      "|[after, 276.txt, ...|  1|\n",
      "|  [up, 060.txt, 374]|  1|\n",
      "|[unless, 048.txt,...|  1|\n",
      "|[later, 048.txt, ...|  1|\n",
      "|[average, 114.txt...|  1|\n",
      "|[has,, 100.txt, 241]|  1|\n",
      "|[after, 470.txt, ...|  3|\n",
      "|[Johnson,, 302.tx...|  1|\n",
      "|[that, 459.txt, 163]|  1|\n",
      "|[\"I've, 303.txt, ...|  1|\n",
      "|[switch, 317.txt,...|  1|\n",
      "|[understand, 317....|  1|\n",
      "|[capability, 115....|  1|\n",
      "|[never,, 049.txt,...|  1|\n",
      "|[trial, 049.txt, ...|  1|\n",
      "|[men's, 075.txt, ...|  4|\n",
      "| [aim, 061.txt, 653]|  1|\n",
      "|[against, 249.txt...|  2|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial7: org.apache.spark.sql.Dataset[((String, String), Double)] = [_1: struct<_1: string, _2: string>, _2: double]\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial7 = trial6.map(x => ((x._1._1, x._1._2), (x._2.toDouble / x._1._3.toDouble) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|                  _2|\n",
      "+--------------------+--------------------+\n",
      "|   [seeing, 262.txt]|0.001503759398496...|\n",
      "|        [F, 262.txt]|0.003007518796992...|\n",
      "|    [after, 276.txt]|0.002197802197802198|\n",
      "|       [up, 060.txt]| 0.00267379679144385|\n",
      "|   [unless, 048.txt]|0.002392344497607...|\n",
      "|    [later, 048.txt]|0.002392344497607...|\n",
      "|  [average, 114.txt]|0.002949852507374...|\n",
      "|     [has,, 100.txt]|0.004149377593360996|\n",
      "|    [after, 470.txt]|0.006172839506172839|\n",
      "| [Johnson,, 302.txt]|0.001358695652173913|\n",
      "|     [that, 459.txt]|0.006134969325153374|\n",
      "|    [\"I've, 303.txt]|  0.0045662100456621|\n",
      "|   [switch, 317.txt]| 0.00234192037470726|\n",
      "|[understand, 317....| 0.00234192037470726|\n",
      "|[capability, 115....|0.001897533206831...|\n",
      "|   [never,, 049.txt]|0.002207505518763797|\n",
      "|    [trial, 049.txt]|0.002207505518763797|\n",
      "|    [men's, 075.txt]|0.008064516129032258|\n",
      "|      [aim, 061.txt]|0.001531393568147...|\n",
      "|  [against, 249.txt]|0.004310344827586207|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial8: org.apache.spark.sql.Dataset[(String, String)] = [_1: string, _2: string]\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial8 = trial7.map(x => (x._1._1, x._1._2.toString.concat(\"( \").concat(x._2.toString).concat(\" )\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|        _1|                  _2|\n",
      "+----------+--------------------+\n",
      "|    seeing|262.txt( 0.001503...|\n",
      "|         F|262.txt( 0.003007...|\n",
      "|     after|276.txt( 0.002197...|\n",
      "|        up|060.txt( 0.002673...|\n",
      "|    unless|048.txt( 0.002392...|\n",
      "|     later|048.txt( 0.002392...|\n",
      "|   average|114.txt( 0.002949...|\n",
      "|      has,|100.txt( 0.004149...|\n",
      "|     after|470.txt( 0.006172...|\n",
      "|  Johnson,|302.txt( 0.001358...|\n",
      "|      that|459.txt( 0.006134...|\n",
      "|     \"I've|303.txt( 0.004566...|\n",
      "|    switch|317.txt( 0.002341...|\n",
      "|understand|317.txt( 0.002341...|\n",
      "|capability|115.txt( 0.001897...|\n",
      "|    never,|049.txt( 0.002207...|\n",
      "|     trial|049.txt( 0.002207...|\n",
      "|     men's|075.txt( 0.008064...|\n",
      "|       aim|061.txt( 0.001531...|\n",
      "|   against|249.txt( 0.004310...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial9: org.apache.spark.sql.DataFrame = [_1: string, concat_ws( , collect_list(_2) AS `_2`): string]\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trial9 = trial8.groupBy('_1).agg(concat_ws(\" \", collect_list(\"_2\") as \"_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------+\n",
      "|           _1|concat_ws( , collect_list(_2) AS `_2`)|\n",
      "+-------------+--------------------------------------+\n",
      "|        still|                  086.txt( 0.005813...|\n",
      "|      elevate|                  170.txt( 0.002012...|\n",
      "|          few|                  302.txt( 0.001358...|\n",
      "|         some|                  356.txt( 0.006920...|\n",
      "|         hope|                  415.txt( 0.003134...|\n",
      "|            K|                  202.txt( 0.002178...|\n",
      "|        those|                  274.txt( 0.002680...|\n",
      "|        cold,|                  072.txt( 0.001785...|\n",
      "|   creativity|                  360.txt( 0.001615...|\n",
      "|    involving|                  112.txt( 0.005128...|\n",
      "|         July|                  307.txt( 0.002915...|\n",
      "|      barrier|                  069.txt( 0.001261...|\n",
      "|     Sunday,\"|                  127.txt( 0.002257...|\n",
      "|        cramp|                  098.txt( 0.005464...|\n",
      "|       'turns|                  414.txt( 0.005524...|\n",
      "|precautionary|                  245.txt( 0.005714...|\n",
      "|    sidelined|                  304.txt( 0.003496...|\n",
      "|  overlooked,|                  348.txt( 0.002272...|\n",
      "|     Previous|                  197.txt( 0.006849...|\n",
      "|      Antwerp|                  390.txt( 0.004132...|\n",
      "+-------------+--------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: Array[org.apache.spark.sql.Row] = Array([still,086.txt( 0.005813953488372093 ) 259.txt( 0.008264462809917356 ) 009.txt( 0.002631578947368421 ) 321.txt( 0.0022222222222222222 ) 155.txt( 0.0035087719298245615 ) 387.txt( 0.002074688796680498 ) 358.txt( 0.004629629629629629 ) 121.txt( 0.002717391304347826 ) 362.txt( 0.001984126984126984 ) 094.txt( 0.0091324200913242 ) 156.txt( 0.003703703703703704 ) 242.txt( 0.003913894324853229 ) 194.txt( 0.0022026431718061676 ) 435.txt( 0.0014858841010401188 ) 349.txt( 0.00398406374501992 ) 057.txt( 0.003911342894393742 ) 318.txt( 0.0028653295128939827 ) 161.txt( 0.0049504950495049506 ) 205.txt( 0.00199203187250996 ) 091.txt( 0.0034129692832764505 ) 270.txt( 0.0026246719160104987 ) 249.txt( 0.0021551724137931034 ) 104.txt( 0.0016501650165016502 ) 2..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial9.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
